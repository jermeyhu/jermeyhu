---
layout: post
title: '利用谷歌colab 训练ChatGLM2'
date: '2023-12-08'
categories: 人工智能
---

## 完整笔记文件

[利用谷歌colab 训练ChatGLM2](https://colab.research.google.com/drive/1WlXojj782-VZ6Rmu8FtGlERhDtoB7lE5?usp=sharing)

ps:由于性能原因，无法训练全部参数。

## 分步说明

### 1. 挂载谷歌硬盘

执行下面代码挂载谷歌硬盘，主要目的是保存训练的结果，保存修改后的代码。

```
from google.colab import drive
drive.mount('/content/drive')
```

### 2. 进入工作目录

通过cd命令进入到挂载的谷歌硬盘的目录,我这里有进入到谷歌硬盘里面名为Code的目录。
在colab执行cd命令需要在前面加%

```
%cd drive/MyDrive/Code
```

### 3. 下载代码

使用git命令将代码下载到Code文件夹内。
在colab执行linux命令需要加!,比如执行apt命令之类的，需要!apt

```
!git clone https://github.com/THUDM/ChatGLM2-6B.git
```

### 4. 切换到代码目录

```
%cd ChatGLM2-6B
```

### 5. 安装依赖

这里执行pip命令，所以前面有加!

```
!pip install -r requirements.txt
```

### 6. 切换到ptuning目录

```
%cd ptuning
```

### 7. 上传准备好的训练资料。

可以参考[训练资料](/file/self_cognition.json)

### 8. 根据训练资料修改`train.sh`

参考

```
PRE_SEQ_LEN=128
LR=2e-2
NUM_GPUS=1

torchrun --standalone --nnodes=1 --nproc-per-node=$NUM_GPUS main.py \
    --do_train \
    --train_file self_cognition.json \
    --validation_file self_cognition.json \
    --preprocessing_num_workers 10 \
    --prompt_column content \
    --response_column summary \
    --overwrite_cache \
    --model_name_or_path THUDM/chatglm2-6b-32k-int4 \
    --output_dir output/adgen-chatglm2-6b-32k-int4-$PRE_SEQ_LEN-$LR \
    --overwrite_output_dir \
    --max_source_length 64 \
    --max_target_length 128 \
    --per_device_train_batch_size 1 \
    --per_device_eval_batch_size 1 \
    --gradient_accumulation_steps 16 \
    --predict_with_generate \
    --max_steps 100 \
    --logging_steps 10 \
    --save_steps 50 \
    --learning_rate $LR \
    --pre_seq_len $PRE_SEQ_LEN \
    --quantization_bit 4

```

### 9. 执行训练

```
!bash train.sh
```

### 10. 下载训练后的参数文件

进入谷歌硬盘，Code目录，找到对应的输出文件下载即可。

(完)